{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<h1>OCR project with Python</h1>\n",
    "</div>\n",
    "\n",
    "<h2>Project Overview</h2>\n",
    "\n",
    "Optical Character Recognition (OCR) is a technology that extracts and converts text from images, scanned documents, or handwritten content into machine-readable formats. It is widely used for digitizing printed materials, automating data entry, and enabling text-based search within visual content.\n",
    "\n",
    "In this project, we utilize the **`easyocr`** library, a lightweight and highly efficient Optical Character Recognition (OCR) solution powered by deep learning techniques. **`easyocr`** supports over 80 languages and uses advanced models like **ResNet** and **LSTM** (Long Short-Term Memory) to extract text with high accuracy, even from challenging images such as low-quality photos or non-standard fonts. \n",
    "\n",
    "In this project, we utilize **Tesseract**, an open-source Optical Character Recognition (OCR) engine. Tesseract is one of the most accurate and widely-used OCR libraries, supporting over 100 languages. It works by recognizing characters in images and converting them into editable text, and is particularly effective for documents and scanned images. With its integration into Python via the `pytesseract` library, Tesseract provides a powerful tool for extracting text from images in various formats, making it an essential choice for many OCR tasks.\n",
    "\n",
    "\n",
    "<h2>LSTM</h2>\n",
    "\n",
    "A Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) designed to process sequential data by capturing both short-term and long-term dependencies. Unlike traditional RNNs, LSTMs use a unique architecture with memory cells and gates (input, forget, and output) that regulate the flow of information. This allows them to retain relevant information over extended sequences while avoiding issues like vanishing gradients, making them ideal for tasks such as speech recognition, text generation, and time-series analysis.\n",
    "\n",
    "![LSTM Scheme](\\lstm_scheme.jpg)\n",
    "\n",
    "\n",
    "Reference: [Understanding LSTM and Its Diagrams](https://blog.mlreview.com/understanding-lstm-and-its-diagrams-37e2f46f1714)\n",
    "\n",
    "<h2>Installation Instructions:</h2>\n",
    "Before running this notebook, you need to install the required library, **EasyOCR**. You can install it by running the following command in a code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install easyocr\n",
    "!pip install scikit-learn\n",
    "!pip install pytesseract\n",
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">EasyOCR</h2>\n",
    "<h3> Naive Approach for OCR Text Extraction </h3>\n",
    "\n",
    "In this section, we demonstrate a **naive approach** to Optical Character Recognition (OCR) using the **EasyOCR** library. The idea is to extract text from images using EasyOCR's built-in methods without any additional optimization or preprocessing. The following steps are involved:\n",
    "\n",
    "1. **Image and Text Files Setup**: We define the paths to the images (`test_OCR_1.jpg` and `test_OCR_2.jpg`) and their corresponding expected translations stored in text files (`test_OCR_trad_1.txt` and `test_OCR_trad_2.txt`).\n",
    "2. **Reading Translations**: We read the translations stored in the text files to compare with the OCR results.\n",
    "3. **OCR Initialization and Execution**: We initialize an EasyOCR reader for both French (`fr`) and English (`en`) and use it to extract text from the two images.\n",
    "4. **Text Extraction and Formatting**: For each image, we extract the detected text and accumulate it in separate variables (`res1_text` and `res2_text`).\n",
    "5. **Saving the Result**: Finally, we append the OCR results to a file (`data.txt`) for later evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "\n",
    "def process_ocr_image_naive(image_path, output_path, languages=['fr', 'en']):\n",
    "    \"\"\"\n",
    "    Processes OCR on a single image and saves the extracted text to an output file.\n",
    "    \n",
    "    Args:\n",
    "    - image_path (str): File path to the input image for OCR processing.\n",
    "    - output_path (str): File path where the extracted text will be saved.\n",
    "    - languages (list of str, optional): List of languages for the OCR reader (default is ['fr', 'en']).\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \n",
    "    Notes:\n",
    "    - This function reads text from the image using EasyOCR and writes the extracted text\n",
    "      to the specified output file in 'utf-8' encoding.\n",
    "    \"\"\"\n",
    "    reader = easyocr.Reader(languages)\n",
    "    \n",
    "    results = reader.readtext(image_path)\n",
    "    \n",
    "    extracted_text = \"\\n\".join([text for _, text, _ in results])\n",
    "    \n",
    "    with open(output_path, \"a\", encoding=\"utf-8\") as file:\n",
    "        file.write(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "test_OCR_1 = \"../OCR_Items/test_OCR_1.jpg\"\n",
    "test_OCR_trad_1 = \"../OCR_Items/test_OCR_trad_1.txt\"\n",
    "test_OCR_2 = \"../OCR_Items/test_OCR_2.jpg\"\n",
    "test_OCR_trad_2 = \"../OCR_Items/test_OCR_trad_2.txt\"\n",
    "path_naive_approach_test_ocr_1 = \"./EasyOCR_Result/naive_test_ocr_1.txt\"\n",
    "path_naive_approach_test_ocr_2 = \"./EasyOCR_Result/naive_test_ocr_2.txt\"\n",
    "\n",
    "with open(test_OCR_trad_1, \"r\",encoding=\"utf-8\") as fichier:\n",
    "\ttraduction_1 = fichier.read()\n",
    "\n",
    "with open(test_OCR_trad_2, \"r\",encoding=\"utf-8\") as fichier:\n",
    "\ttraduction_2 = fichier.read()\n",
    "\n",
    "\n",
    "process_ocr_image_naive(test_OCR_1, path_naive_approach_test_ocr_1)\n",
    "\n",
    "process_ocr_image_naive(test_OCR_2, path_naive_approach_test_ocr_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Comparing OCR Text with Reference Text </h3>\n",
    "\n",
    "In this section, we will create a function that compares the output from the Optical Character Recognition (OCR) with a reference text. The goal is to evaluate how similar the extracted text is to the original text. \n",
    "\n",
    "We will use a **Cosine Similarity** approach to measure the percentage of similarity between the two texts. Cosine Similarity calculates the cosine of the angle between two vectors, representing the two texts. A higher cosine similarity value indicates that the two texts are more alike.\n",
    "\n",
    "To transform the texts into numerical vectors, we will use the **TF-IDF** (Term Frequency-Inverse Document Frequency) method. TF-IDF is a statistical measure used to evaluate how important a word is in a document relative to a collection of documents (or corpus). It helps in reducing the influence of commonly occurring words (like \"the\", \"is\", etc.) while emphasizing the more meaningful words in the text.\n",
    "\n",
    "- **Term Frequency (TF)**: Measures how frequently a term appears in a document.\n",
    "- **Inverse Document Frequency (IDF)**: Measures how important a term is by calculating the inverse of how often it appears across all documents.\n",
    "\n",
    "For more detailed information about TF-IDF, you can refer to this [Wikipedia page](https://fr.wikipedia.org/wiki/TF-IDF).\n",
    "\n",
    "By the end of this comparison, we will have a similarity percentage that indicates how closely the OCR text matches the reference text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compare_texts(refText, textToCompare):\n",
    "    \"\"\"\n",
    "    Compares two texts and calculates their similarity percentage using \n",
    "    the Cosine Similarity and TF-IDF (Term Frequency-Inverse Document Frequency) method.\n",
    "    \n",
    "    Args:\n",
    "    - refText (str): Reference text.\n",
    "    - textToCompare (str): Text to compare with the reference text.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The percentage similarity between the two texts (0 to 100).\n",
    "    \n",
    "    Notes:\n",
    "    - This version handles multi-line texts by converting them into a single uniform line.\n",
    "    - This version converts both texts to lowercase for case-insensitive comparison.\n",
    "    - This version does not use any stop words for comparison.\n",
    "    \"\"\"\n",
    "    \n",
    "    refText = \" \".join(refText.splitlines()).lower()\n",
    "    textToCompare = \" \".join(textToCompare.splitlines()).lower()\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([refText, textToCompare])\n",
    "\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
    "\n",
    "    similarity_percentage = round(cosine_sim[0][0] * 100, 2)\n",
    "\n",
    "    return similarity_percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity rate for text 1 is : 1.67 / 100\n",
      "The similarity rate for text 2 is : 0.0 / 100\n"
     ]
    }
   ],
   "source": [
    "with open(path_naive_approach_test_ocr_1, \"r\",encoding=\"utf-8\") as fichier:\n",
    "\tnaive_approach_test_ocr_1 = fichier.read()\n",
    "\n",
    "with open(path_naive_approach_test_ocr_1, \"r\",encoding=\"utf-8\") as fichier:\n",
    "\tnaive_approach_test_ocr_2 = fichier.read()\n",
    "\n",
    "print(f\"The similarity rate for text 1 is : {compare_texts(traduction_1,naive_approach_test_ocr_1)} / 100\")\n",
    "print(f\"The similarity rate for text 2 is : {compare_texts(traduction_2,naive_approach_test_ocr_2)} / 100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Processing Noisy Images with OCR </h3>\n",
    "\n",
    "<h4>Objective</h4>\n",
    "This section focuses on handling noisy images to extract text using Optical Character Recognition (OCR). Noise in images, such as graininess or blurriness, can hinder OCR accuracy. To address this, we preprocess the images by reducing noise before performing OCR.\n",
    "\n",
    "<h4> Steps Involved </h4>\n",
    "1. <b>Reading the Image</b>:\n",
    "   - The noisy image is loaded in grayscale using OpenCV. Grayscale simplifies the image processing pipeline by working with a single color channel.\n",
    "\n",
    "2. <b>Reducing Noise</b>:\n",
    "   - A <b>Gaussian Blur</b> filter is applied to the image. This filter smoothens the image by reducing high-frequency noise, which can significantly improve OCR performance.\n",
    "\n",
    "3. <b>Text Extraction with EasyOCR</b>:\n",
    "   - We use EasyOCR, a robust and multilingual OCR library, to detect and extract text from the preprocessed image. EasyOCR works with multiple languages and returns both the detected text and its confidence score.\n",
    "\n",
    "4. <b>Saving the Extracted Text</b>:\n",
    "   - The extracted text is written to a file for further analysis or use. This ensures that the results are reproducible and accessible for subsequent steps in the pipeline.\n",
    "\n",
    "<h4> Why Use Gaussian Blur? </h4>\n",
    "Gaussian blur is a widely used preprocessing technique in computer vision. It helps:<br>\n",
    "- Reduce image noise and detail, making text regions stand out more clearly.<br>\n",
    "- Improve the accuracy of OCR by eliminating small, irrelevant artifacts in the image.<br>\n",
    "\n",
    "<h4> Example Use Case </h4>\n",
    "Imagine you have a noisy scanned document or a photograph of a sign with visual artifacts. By applying this preprocessing approach:<br>\n",
    "- You reduce the noise using Gaussian blur.<br>\n",
    "- Extract the text using OCR with higher accuracy.<br>\n",
    "- Save the transcription for further use, such as comparison with reference texts.<br>\n",
    "\n",
    "<h4> Code Explanation </h4>\n",
    "The function `process_noisy_image` takes:\n",
    "- The path to the input image.<br>\n",
    "- The path to save the extracted text.\n",
    "- Optional parameters for specifying OCR languages and Gaussian blur settings.<br>\n",
    "\n",
    "This function ensures the noisy image is processed efficiently and provides a clean transcription of the text contained in the image.\n",
    "\n",
    "<h4>References</h4>\n",
    "<ul>\n",
    "    <li>EasyOCR Documentation: <a href=\"https://www.jaided.ai/easyocr/\" target=\"_blank\">https://www.jaided.ai/easyocr/</a></li>\n",
    "    <li>Gaussian Blur Explanation: <a href=\"https://en.wikipedia.org/wiki/Gaussian_blur\" target=\"_blank\">https://en.wikipedia.org/wiki/Gaussian_blur</a></li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import easyocr\n",
    "\n",
    "def process_noisy_image(input_image_path, output_text_path, languages=['en'], blur_kernel=(5, 5)):\n",
    "    \"\"\"\n",
    "    Processes a noisy image by applying Gaussian blur and performs OCR to extract text,\n",
    "    saving the result to a text file.\n",
    "\n",
    "    Args:\n",
    "    - input_image_path (str): Path to the input image file.\n",
    "    - output_text_path (str): Path to the output text file where OCR results will be saved.\n",
    "    - languages (list of str, optional): List of languages for OCR processing (default is ['en']).\n",
    "    - blur_kernel (tuple, optional): Kernel size for Gaussian blur (default is (5, 5)).\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "\n",
    "    Notes:\n",
    "    - The function reads the image, applies Gaussian blur to reduce noise, and performs OCR.\n",
    "    - The extracted text is saved in UTF-8 encoding to the specified output file.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(input_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Image not found at path: {input_image_path}\")\n",
    "    \n",
    "    blurred_img = cv2.GaussianBlur(img, blur_kernel, 0)\n",
    "    \n",
    "    reader = easyocr.Reader(languages)\n",
    "    \n",
    "    results = reader.readtext(blurred_img)\n",
    "    \n",
    "    extracted_text = \"\\n\".join([text for _, text, _ in results])\n",
    "    \n",
    "    with open(output_text_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity rate for text 1 is : 4.6 / 100\n",
      "The similarity rate for text 2 is : 0.0 / 100\n"
     ]
    }
   ],
   "source": [
    "path_GB_approach_test_ocr_1 = \"./EasyOCR_Result/GB_test_ocr_1.txt\"\n",
    "path_GB_approach_test_ocr_2 = \"./EasyOCR_Result/GB_test_ocr_2.txt\"\n",
    "\n",
    "process_noisy_image(test_OCR_1, path_GB_approach_test_ocr_1)\n",
    "process_noisy_image(test_OCR_2, path_GB_approach_test_ocr_2)\n",
    "\n",
    "with open(path_GB_approach_test_ocr_1, \"r\",encoding=\"utf-8\") as fichier:\n",
    "\tnoisy_image_ocr_1 = fichier.read()\n",
    "\n",
    "with open(path_GB_approach_test_ocr_2, \"r\",encoding=\"utf-8\") as fichier:\n",
    "\tnoisy_image_ocr_2 = fichier.read()\n",
    "\n",
    "print(f\"The similarity rate for text 1 is : {compare_texts(traduction_1,noisy_image_ocr_1)} / 100\")\n",
    "print(f\"The similarity rate for text 2 is : {compare_texts(traduction_2,noisy_image_ocr_2)} / 100\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Improvement </h3>\n",
    "\n",
    "The goal of this function is to:<br>\n",
    "1. Load an image containing handwritten text.<br>\n",
    "2. Apply image preprocessing to make the text more readable for OCR (Optical Character Recognition).<br>\n",
    "3. Use <b>EasyOCR</b> to extract the text.<br>\n",
    "4. Save the extracted text to a file.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import easyocr\n",
    "\n",
    "def preprocess_and_process_image(input_image_path, output_text_path):\n",
    "    \"\"\"\n",
    "    Preprocesses an image of handwritten text and extracts the text using EasyOCR.\n",
    "\n",
    "    Args:\n",
    "    - input_image_path (str): Path to the input image.\n",
    "    - output_text_path (str): Path to save the transcribed text.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "\n",
    "    Steps:\n",
    "    1. Load the image and preprocess it for handwritten text.\n",
    "    2. Apply EasyOCR to extract text from the processed image.\n",
    "    3. Save the extracted text to the specified file.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(input_image_path, 0)\n",
    "    \n",
    "    binary_img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                       cv2.THRESH_BINARY, 11, 2)\n",
    "    \n",
    "    denoised_img = cv2.medianBlur(binary_img, 3)\n",
    "    \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "    processed_img = cv2.morphologyEx(denoised_img, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "\n",
    "    reader = easyocr.Reader(['en', 'fr'])\n",
    "    results = reader.readtext(processed_img)\n",
    "\n",
    "    extracted_text = \"\"\n",
    "    for (_, text, _) in results:\n",
    "        extracted_text += text + \"\\n\"\n",
    "\n",
    "    with open(output_text_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        output_file.write(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity rate for text 1 is : 0.0 / 100\n",
      "The similarity rate for text 2 is : 0.0 / 100\n"
     ]
    }
   ],
   "source": [
    "path_PPI_approach_test_ocr_1 = \"./EasyOCR_Result/PPI_test_ocr_1.txt\"\n",
    "path_PPI_approach_test_ocr_2 = \"./EasyOCR_Result/PPI_test_ocr_2.txt\"\n",
    "\n",
    "preprocess_and_process_image(test_OCR_1, path_PPI_approach_test_ocr_1)\n",
    "preprocess_and_process_image(test_OCR_2, path_PPI_approach_test_ocr_2)\n",
    "\n",
    "with open(path_PPI_approach_test_ocr_1, \"r\",encoding=\"utf-8\") as fichier:\n",
    "\tppi_image_ocr_1 = fichier.read()\n",
    "\n",
    "with open(path_PPI_approach_test_ocr_2, \"r\",encoding=\"utf-8\") as fichier:\n",
    "\tppi_image_ocr_2 = fichier.read()\n",
    "\n",
    "print(f\"The similarity rate for text 1 is : {compare_texts(traduction_1,ppi_image_ocr_1)} / 100\")\n",
    "print(f\"The similarity rate for text 2 is : {compare_texts(traduction_2,ppi_image_ocr_2)} / 100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Testing on a Non-Handwritten Image </h3>\n",
    "\n",
    "In this section, we will perform a test using an image that does not contain handwritten text, but rather printed text. This will allow us to observe the differences in the OCR results when applied to non-handwritten text.\n",
    "\n",
    "<h4> Objective </h4>\n",
    "The goal of this test is to compare the performance of the EasyOCR model on:<br>\n",
    "1. <b>Handwritten text images</b>: Which we have been processing in the previous steps.<br>\n",
    "2. <b>Printed text images</b>: To observe how the preprocessing steps affect the recognition accuracy when the text is not handwritten but printed.<br>\n",
    "\n",
    "<h4> Expected EasyOCR_Result </h4>\n",
    "- <b>Handwritten text</b>: Handwritten text often varies in style, spacing, and clarity, which can make recognition more difficult for OCR models, even with preprocessing techniques.<br>\n",
    "- <b>Printed text</b>: Printed text, being more uniform and clear, is typically easier for OCR models to recognize, and we expect better results with less need for heavy preprocessing.<br>\n",
    "\n",
    "By running the OCR on a printed text image, we will be able to directly compare the effectiveness of our preprocessing steps on different types of text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive approach : 81.02 / 100\n",
      "Gaussian Blur : 74.08 / 100\n",
      "Preprocess : 88.2 / 100\n"
     ]
    }
   ],
   "source": [
    "path_trad = \"../OCR_Items/test_image.txt\"\n",
    "test_OCR_3 = \"../OCR_Items/test_image.jpeg\"\n",
    "\n",
    "\n",
    "with open(path_trad, \"r\",encoding=\"utf-8\") as fichier:\n",
    "\ttraduction_3 = fichier.read()\n",
    "\n",
    "\n",
    "path_NHW_naive_approach_test_ocr = \"./EasyOCR_Result/NHW_naive_test_ocr.txt\"\n",
    "path_NHW_GB_approach_test_ocr = \"./EasyOCR_Result/NHW_GB_test_ocr.txt\"\n",
    "path_NHW_PPI_approach_test_ocr = \"./EasyOCR_Result/NHW_PPI_test_ocr.txt\"\n",
    "\n",
    "process_ocr_image_naive(test_OCR_3, path_NHW_naive_approach_test_ocr)\n",
    "process_noisy_image(test_OCR_3, path_NHW_GB_approach_test_ocr)\n",
    "preprocess_and_process_image(test_OCR_3, path_NHW_PPI_approach_test_ocr)\n",
    "\n",
    "with open(path_trad, \"r\",encoding=\"utf-8\") as fichier:\n",
    "\ttraduction_3 = fichier.read()\n",
    "\t\n",
    "with open(path_NHW_naive_approach_test_ocr, \"r\",encoding=\"utf-8\") as fichier:\n",
    "\tNHW_Naive = fichier.read()\n",
    "\n",
    "with open(path_NHW_GB_approach_test_ocr, \"r\",encoding=\"utf-8\") as fichier:\n",
    "\tNHW_GB = fichier.read()\n",
    "\n",
    "with open(path_NHW_PPI_approach_test_ocr, \"r\",encoding=\"utf-8\") as fichier:\n",
    "\tNHW_PPI = fichier.read()\n",
    "\n",
    "print(f\"Naive approach : {compare_texts(traduction_3,NHW_Naive)} / 100\")\n",
    "print(f\"Gaussian Blur : {compare_texts(traduction_3,NHW_GB)} / 100\")\n",
    "print(f\"Preprocess : {compare_texts(traduction_3,NHW_PPI)} / 100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>OCR with Tesseract</h2>\n",
    "\n",
    "In the previous section, we used **EasyOCR**, a lightweight and efficient library for Optical Character Recognition (OCR). This time, we will perform the same task using **Tesseract**, one of the most popular and powerful open-source OCR engines.\n",
    "\n",
    "The goal is to compare the performance of these two libraries to determine which one delivers the best results in terms of accuracy, speed, and flexibility under various conditions (e.g., low-quality images, handwritten text, etc.).\n",
    "\n",
    "We will follow the same process as before by extracting text from an image and saving it to an output file. However, this time, we will use **`pytesseract`**, a Python wrapper for **Tesseract**, to perform the OCR on the images.\n",
    "\n",
    "By comparing the results from **EasyOCR** and **Tesseract**, we will gain a better understanding of the strengths and limitations of each library and make an informed decision for future OCR tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "def process_ocr_image_naive_tesseract(image_path, output_path, languages=['fra', 'eng']):\n",
    "    \"\"\"\n",
    "    Processes OCR on a single image and saves the extracted text to an output file.\n",
    "    \n",
    "    Args:\n",
    "    - image_path (str): File path to the input image for OCR processing.\n",
    "    - output_path (str): File path where the extracted text will be saved.\n",
    "    - languages (list of str, optional): List of languages for the OCR reader (default is ['fra', 'eng']).\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \n",
    "    Notes:\n",
    "    - This function reads text from the image using Tesseract and writes the extracted text\n",
    "      to the specified output file in 'utf-8' encoding.\n",
    "    \"\"\"\n",
    "    \n",
    "    pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe' \n",
    "    \n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    extracted_text = pytesseract.image_to_string(img, lang='+'.join(languages))\n",
    "    \n",
    "    with open(output_path, \"a\", encoding=\"utf-8\") as file:\n",
    "        file.write(extracted_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "def process_noisy_image_tesseract(input_image_path, output_text_path, languages=['eng'], blur_kernel=(5, 5)):\n",
    "    \"\"\"\n",
    "    Processes a noisy image by applying Gaussian blur and performs OCR to extract text,\n",
    "    saving the result to a text file using Tesseract OCR.\n",
    "\n",
    "    Args:\n",
    "    - input_image_path (str): Path to the input image file.\n",
    "    - output_text_path (str): Path to the output text file where OCR results will be saved.\n",
    "    - languages (list of str, optional): List of languages for OCR processing (default is ['eng']).\n",
    "    - blur_kernel (tuple, optional): Kernel size for Gaussian blur (default is (5, 5)).\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "\n",
    "    Notes:\n",
    "    - The function reads the image, applies Gaussian blur to reduce noise, and performs OCR using Tesseract.\n",
    "    - The extracted text is saved in UTF-8 encoding to the specified output file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the image in grayscale\n",
    "    img = cv2.imread(input_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Image not found at path: {input_image_path}\")\n",
    "    \n",
    "    # Apply Gaussian blur to reduce noise\n",
    "    blurred_img = cv2.GaussianBlur(img, blur_kernel, 0)\n",
    "    \n",
    "    # Convert the image from OpenCV format (BGR) to Pillow format (RGB)\n",
    "    pil_img = Image.fromarray(blurred_img)\n",
    "    \n",
    "    # Perform OCR using Tesseract\n",
    "    extracted_text = pytesseract.image_to_string(pil_img, lang='+'.join(languages))\n",
    "    \n",
    "    # Save the extracted text to a file\n",
    "    with open(output_text_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_and_process_image_tesseract(input_image_path, output_text_path):\n",
    "    \"\"\"\n",
    "    Preprocesses an image of handwritten text and extracts the text using Tesseract OCR.\n",
    "\n",
    "    Args:\n",
    "    - input_image_path (str): Path to the input image.\n",
    "    - output_text_path (str): Path to save the transcribed text.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "\n",
    "    Steps:\n",
    "    1. Load the image and preprocess it for handwritten text.\n",
    "    2. Apply Tesseract OCR to extract text from the processed image.\n",
    "    3. Save the extracted text to the specified file.\n",
    "    \"\"\"\n",
    "    \n",
    "    img = cv2.imread(input_image_path, 0)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Image not found at path: {input_image_path}\")\n",
    "    \n",
    "    binary_img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                       cv2.THRESH_BINARY, 11, 2)\n",
    "    \n",
    "    denoised_img = cv2.medianBlur(binary_img, 3)\n",
    "    \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "    processed_img = cv2.morphologyEx(denoised_img, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    pil_img = Image.fromarray(processed_img)\n",
    "    \n",
    "    extracted_text = pytesseract.image_to_string(pil_img, lang='eng+fra')\n",
    "    \n",
    "    with open(output_text_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        output_file.write(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_OCR_1 = \"../OCR_Items/test_OCR_1.jpg\"\n",
    "test_OCR_2 = \"../OCR_Items/test_OCR_2.jpg\"\n",
    "test_OCR_3 = \"../OCR_Items/test_image.jpeg\"\n",
    "\n",
    "test_OCR_trad_1 = \"../OCR_Items/test_OCR_trad_1.txt\"\n",
    "test_OCR_trad_2 = \"../OCR_Items/test_OCR_trad_2.txt\"\n",
    "test_OCR_trad_3 = \"../OCR_Items/test_image.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity rate for text 1 is : 0.0 / 100\n",
      "The similarity rate for text 2 is : 0.0 / 100\n",
      "The similarity rate for text 2 is : 95.96 / 100\n"
     ]
    }
   ],
   "source": [
    "path_naive_approach_test_ocr_1 = \"./Tesseract_Result/naive_test_ocr_1.txt\"\n",
    "path_naive_approach_test_ocr_2 = \"./Tesseract_Result/naive_test_ocr_2.txt\"\n",
    "path_NHW_naive_approach_test_ocr = \"./Tesseract_Result/NHW_naive_test_ocr.txt\"\n",
    "\n",
    "process_ocr_image_naive_tesseract(test_OCR_1,path_naive_approach_test_ocr_1)\n",
    "process_ocr_image_naive_tesseract(test_OCR_2,path_naive_approach_test_ocr_2)\n",
    "process_ocr_image_naive_tesseract(test_OCR_3,path_NHW_naive_approach_test_ocr)\n",
    "\n",
    "with open(path_naive_approach_test_ocr_1, \"r\",encoding=\"utf-8\") as fichier:\n",
    "\ttest_1 = fichier.read()\n",
    "\n",
    "with open(path_naive_approach_test_ocr_2, \"r\",encoding=\"utf-8\") as fichier:\n",
    "\ttest_2 = fichier.read()\n",
    "\n",
    "with open(path_NHW_naive_approach_test_ocr, \"r\",encoding=\"utf-8\") as fichier:\n",
    "\ttest_3 = fichier.read()\n",
    "\n",
    "print(f\"The similarity rate for text 1 is : {compare_texts(traduction_1,test_1)} / 100\")\n",
    "print(f\"The similarity rate for text 2 is : {compare_texts(traduction_2,test_2)} / 100\")\n",
    "print(f\"The similarity rate for text 2 is : {compare_texts(traduction_3,test_3)} / 100\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity rate for text 1 is : 0.0 / 100\n",
      "The similarity rate for text 2 is : 0.0 / 100\n",
      "The similarity rate for text 2 is : 95.96 / 100\n"
     ]
    }
   ],
   "source": [
    "path_GB_approach_test_ocr_1 = \"./Tesseract_Result/GB_test_ocr_1.txt\"\n",
    "path_GB_approach_test_ocr_2 = \"./Tesseract_Result/GB_test_ocr_2.txt\"\n",
    "path_NHW_GB_approach_test_ocr = \"./Tesseract_Result/NHW_GB_test_ocr.txt\"\n",
    "\n",
    "process_noisy_image_tesseract(test_OCR_1,path_GB_approach_test_ocr_1)\n",
    "process_noisy_image_tesseract(test_OCR_2,path_GB_approach_test_ocr_2)\n",
    "process_noisy_image_tesseract(test_OCR_3,path_NHW_GB_approach_test_ocr)\n",
    "\n",
    "with open(path_GB_approach_test_ocr_1, \"r\",encoding=\"utf-8\") as fichier:\n",
    "\ttest_1 = fichier.read()\n",
    "\n",
    "with open(path_GB_approach_test_ocr_2, \"r\",encoding=\"utf-8\") as fichier:\n",
    "\ttest_2 = fichier.read()\n",
    "\n",
    "with open(path_NHW_GB_approach_test_ocr, \"r\",encoding=\"utf-8\") as fichier:\n",
    "\ttest_3 = fichier.read()\n",
    "\n",
    "print(f\"The similarity rate for text 1 is : {compare_texts(traduction_1,test_1)} / 100\")\n",
    "print(f\"The similarity rate for text 2 is : {compare_texts(traduction_2,test_2)} / 100\")\n",
    "print(f\"The similarity rate for text 2 is : {compare_texts(traduction_3,test_3)} / 100\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity rate for text 1 is : 0.0 / 100\n",
      "The similarity rate for text 2 is : 0.0 / 100\n",
      "The similarity rate for text 2 is : 95.96 / 100\n"
     ]
    }
   ],
   "source": [
    "path_PPI_approach_test_ocr_1 = \"./Tesseract_Result/PPI_test_ocr_1.txt\"\n",
    "path_PPI_approach_test_ocr_2 = \"./Tesseract_Result/PPI_test_ocr_2.txt\"\n",
    "path_NHW_PPI_approach_test_ocr = \"./Tesseract_Result/NHW_PPI_test_ocr.txt\"\n",
    "\n",
    "preprocess_and_process_image_tesseract(test_OCR_1,path_PPI_approach_test_ocr_1)\n",
    "preprocess_and_process_image_tesseract(test_OCR_2,path_PPI_approach_test_ocr_2)\n",
    "preprocess_and_process_image_tesseract(test_OCR_3,path_NHW_PPI_approach_test_ocr)\n",
    "\n",
    "with open(path_PPI_approach_test_ocr_1, \"r\",encoding=\"utf-8\") as fichier:\n",
    "\ttest_1 = fichier.read()\n",
    "\n",
    "with open(path_PPI_approach_test_ocr_2, \"r\",encoding=\"utf-8\") as fichier:\n",
    "\ttest_2 = fichier.read()\n",
    "\n",
    "with open(path_NHW_PPI_approach_test_ocr, \"r\",encoding=\"utf-8\") as fichier:\n",
    "\ttest_3 = fichier.read()\n",
    "\n",
    "print(f\"The similarity rate for text 1 is : {compare_texts(traduction_1,test_1)} / 100\")\n",
    "print(f\"The similarity rate for text 2 is : {compare_texts(traduction_2,test_2)} / 100\")\n",
    "print(f\"The similarity rate for text 2 is : {compare_texts(traduction_3,test_3)} / 100\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
